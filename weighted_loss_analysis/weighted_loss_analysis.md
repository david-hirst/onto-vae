# Onto-VAE with reconstruction loss weights

The object of this project was to allow the optional inclusion of input node weightings for the reconstruction loss of OntoVAE. We have implemented this in the [vae_model.py](https://github.com/david-hirst/onto-vae/blob/main/onto_vae/vae_model.py) module in this forked repository. We have included an argument `rec_loss_wts` in the function which is called to train Onto-VAE 
```
def train_model(self, modelpath, lr=1e-4, kl_coeff=1e-4, batch_size=128, epochs=300, run=None, rec_loss_wts=None):
```
If the user wants to weight the relative contributions of input nodes to the reconstruction loss, they enter a path to a csv file containing the weights, for example
```
ontovae_model.train_model(
modelpath = os.path.join(os.getcwd(),'best_model.pt'),
rec_loss_wts = os.path.join(os.getcwd(),'gene_weights.csv')
)
```
The first column of the csv should contain gene symbols consistant with those used in the loaded ontology object. The second column should contain a weight for each gene. The function expects the csv file to contain column headings, although the headings themselves are ignored. If the user ignores this argument then the training is performed without any node weighting.

## Background

OntoVAE is a variational autoencoder (VAE) that allows for the incorporation of any biological ontology that is a directed acyclic graph (DAG). 

A VAE consists of two neural networks, an encoder and a decoder. The input for the encoder is $\boldsymbol{x}$, which is a $D$-dimensional vector whose elements are the input nodes. The encoder derives $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})$, where $\boldsymbol{z}$ is a vector whose $K$ elements are the nodes in the latent layer. The decoder takes the elements of $\boldsymbol{z}$ as input nodes and derives $\hat{\boldsymbol{x}}$, which is a reconstruction of $\boldsymbol{x}$. 

In OntoVAE, the structure of the decoder is based on a DAG. Each node in the decoder, including those in the latent layer, represents a vertex in the DAG, and connections between decoder nodes are only possible if consistent with the edges between vertices in the DAG. Constraining the decoder in this way allows for interpretability of the decoder node values generated for a given $\boldsymbol{x}$.

We hypothesised that the meaningfulness of the latent values generated by OntoVAE could be improved by weighting the contribution of each input node to the OntoVAE loss function. 

## Weighted reconstruction loss

The contribution of $\boldsymbol{x}$ to the OntoVAE loss function is 
$$L(\boldsymbol{x}, \theta, \phi) = \lambda \times D_{KL}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})||p(\boldsymbol{z})) + \sum_{d=1}^D (x_d - \hat{x}_d)^2$$

Underlying this is the assumption of independent gaussian input nodes, each characterised by $p(x_d|\boldsymbol{z}) = N(\hat{x}_d,\sigma^2)$.

If input node weightings are specified when training OntoVAE, the loss function incorporates a $D$-dimensional vector of weights, $\boldsymbol{w}$: 
$$L_W(\boldsymbol{x}, \theta, \phi) = \lambda \times D_{KL}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})||p(\boldsymbol{z})) + \sum_{d=1}^D w_d(x_d - \hat{x}_d)^2$$
This is equivalent to assuming $p(x_d|\boldsymbol{z}) = N(\hat{x}_d,\nu_d\sigma^2)$ where $w_d = 1/\nu_d$

In our implementation, the user supplies a vector of raw weights $\boldsymbol{r}$, which we normalize to give 
$$w_d =  \frac{r_d \times D}{\sum r_d}$$

## Evaluation

The empirical cumulative distribution of AUC scores

<img src="images/AUC-EDCF-plots.png">

Boxplots of the AUC values

<img src="images/AUC_boxplots.png">
