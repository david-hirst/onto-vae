# Onto-VAE with reconstruction loss weights

The object of this project was to allow the optional inclusion of input node weightings for the reconstruction loss of OntoVAE. We have implemented this in the [vae_model.py](https://github.com/david-hirst/onto-vae/blob/main/onto_vae/vae_model.py) module in this forked repository. We have included an argument `rec_loss_wts` in the function which is called to train Onto-VAE 
```
def train_model(self, modelpath, lr=1e-4, kl_coeff=1e-4, batch_size=128, epochs=300, run=None, rec_loss_wts=None):
```
If the user wants to weight the relative contributions of input nodes to the reconstruction loss, they enter a path to a csv file containing the weights, for example
```
ontovae_model.train_model(
modelpath = os.path.join(os.getcwd(),'best_model.pt'),
rec_loss_wts = os.path.join(os.getcwd(),'gene_weights.csv')
)
```
The first column of the csv should contain gene symbols consistant with those used in the loaded ontology object. The second column should contain a weight for each gene. The function expects the csv file to contain column headings, although the headings themselves are ignored. If the user ignores this argument then the training is performed without any node weighting.

## Background

OntoVAE is a variational autoencoder (VAE) that allows for the incorporation of any biological ontology that is a directed acyclic graph (DAG). 

A VAE consists of two neural networks, an encoder and a decoder. The input for the encoder is $\boldsymbol{x}$, which is a $D$-dimensional vector whose elements are the input nodes. The encoder derives $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})$, where $\boldsymbol{z}$ is a vector whose $K$ elements are the nodes in the latent layer. The decoder takes the elements of $\boldsymbol{z}$ as input nodes and derives $\hat{\boldsymbol{x}}$, which is a reconstruction of $\boldsymbol{x}$. 

In OntoVAE, the structure of the decoder is based on a DAG. Each node in the decoder, including those in the latent layer, represents a vertex in the DAG, and connections between decoder nodes are only possible if consistent with the edges between vertices in the DAG. Constraining the decoder in this way allows for interpretability of the decoder node values generated for a given $\boldsymbol{x}$.

We hypothesised that the meaningfulness of the latent values generated by OntoVAE could be improved by weighting the contribution of each input node to the OntoVAE loss function. 

## Weighted reconstruction loss

For a dataset $\boldsymbol{X}$, the contribution of the $i^{th}$ sample to the OntoVAE loss function is 
$$L(\boldsymbol{x}^{(i)}, \theta, \phi) = \lambda \times D_{KL}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x}^{(i)})||p(\boldsymbol{z})) + \sum_{d=1}^D (x^{(i)}_d - \hat{x}^{(i)}_d)^2$$

Underlying this is the assumption of independent gaussian input nodes, each characterised by $p(x^{(i)}_d|\boldsymbol{z}^{(i)}) = N(\hat{x}^{(i)}_d,\sigma^2)$.

If input node weightings are specified when training OntoVAE, the loss function incorporates a $D$-dimensional vector of weights, $\boldsymbol{w}$: 
$$L_W(\boldsymbol{x}^{(i)}, \theta, \phi) = \lambda \times D_{KL}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x}^{(i)})||p(\boldsymbol{z})) + \sum_{d=1}^D w_d(x^{(i)}_d - \hat{x}^{(i)}_d)^2$$
This is equivalent to assuming $p(x^{(i)}_d|\boldsymbol{z}) = N(\hat{x}^{(i)}_d,\nu_d\sigma^2)$, where $w_d = 1/\nu_d$

In our implementation, the user supplies a vector of raw weights $\boldsymbol{r}$, which we normalize to give 
$$w_d =  \frac{r_d \times D}{\sum r_d}$$

## Evaluation

We evaluated our implementation by finetuning an exisitng OntoVAE model. The exisitng model used a trimmed version of the Gene Ontology (GO) and had been trained on GTEx expression data. In this conext, each input node is a gene and the decoder terms represent terms from the GO.

We created four subsets of the full GTEx expression dataset for calculating weights and finetuning the model. Each subset contained RNA-seq values for samples from one of two tissues. The four tissues type pairs were:
- liver and spleen
- brain and pancreas
- heart and muscle
- adipose tissue and breast
For each tissue pair, we performed a differential expression analysis with DESeq2. We used the absolute vlaue of the moderated log2 fold change between the two tissue types as the weight for each gene.

For each subset, we trained the exisitng OntoVAE model for a further 100 epochs, using only the subset as input data as well as the corresponing derived weights. We also carried out finetuning without weights, as well as with a vector of randomly generated weights. Each finetuning run was carried out independently of the others, meaning only the initial trained model was used as a starting point.

We evaluated each finetuned model by generating decoder node values for each sample. For each decoder node, we trained a seperate naive Bayes classifier with 10-fold cross validation and computed the median area under the curve (AUC). The median AUC indicates how useful a node was for the classification of samples with respect to the two tissue types.    

The empirical cumulative distribution of AUC scores

<img src="images/AUC-EDCF-plots.png">

Boxplots of the AUC values

<img src="images/AUC_boxplots.png">
